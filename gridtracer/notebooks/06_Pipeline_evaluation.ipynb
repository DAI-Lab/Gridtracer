{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gridtracer Data Processing Pipeline Evaluation Notebook\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook aims to provide an evaluation and analysis of the gridtracer data processing pipeline outputs. It corresponds to the 5 major processing steps in `main.py` and serves two primary purposes:\n",
    "\n",
    "1. Validate that each pipeline step produces expected outputs with reasonable data quality\n",
    "2. Generate comprehensive statistics and visualizations for pipeline runs\n",
    "\n",
    "## Pipeline Steps Evaluated\n",
    "\n",
    "Based on `run_pipeline_v2()` in `main.py`, this notebook evaluates:\n",
    "\n",
    "1. **Census Data Analysis** (Step 1) - Regional boundaries, blocks, demographics\n",
    "2. **NREL Data Analysis** (Step 2) - Building characteristics and vintage distributions  \n",
    "3. **OSM Data Analysis** (Step 3) - OpenStreetMap features and infrastructure\n",
    "4. **Building Data Analysis** (Steps 3.5 & 4) - Microsoft Buildings + Classification Heuristics\n",
    "5. **Road Network Analysis** (Step 5) - Routable network generation and topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from shapely.geometry import box\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Configuration - Update these paths based on your output directory structure\n",
    "BASE_OUTPUT_PATH = Path(\"/Users/magic-rabbit/Documents/00_Tech-Repositories/05_MASTER_THESIS/gridtracer/gridtracer/data/output\")  # Update this path\n",
    "REGION_PATH = BASE_OUTPUT_PATH / \"MA\" / \"Middlesex_County\" / \"Cambridge_city\"  # Update based on your region\n",
    "\n",
    "print(\"gridtracer Pipeline Evaluation Notebook Initialized\")\n",
    "print(f\"Base output path: {BASE_OUTPUT_PATH}\")\n",
    "print(f\"Region path: {REGION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Census Data Analysis\n",
    "\n",
    "This section evaluates the census data processing from Step 1 of the pipeline, which handles:\n",
    "- Regional boundary definition\n",
    "- Census block extraction\n",
    "- Demographic data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_census_data(region_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Load and validate census data outputs.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing loaded census data and validation results\n",
    "    \"\"\"\n",
    "    census_path = region_path / \"CENSUS\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Resolve the first match from the generator\n",
    "        blocks_file = next(census_path.glob(\"target_region_blocks.geojson\"))\n",
    "        boundary_file = next(census_path.glob(\"target_region_boundary.geojson\"))\n",
    "\n",
    "        \n",
    "        results['blocks'] = gpd.read_file(blocks_file)\n",
    "        results['boundary'] = gpd.read_file(boundary_file)\n",
    "        results['blocks_file'] = blocks_file\n",
    "        results['boundary_file'] = boundary_file\n",
    "        \n",
    "        print(f\"‚úì Census data loaded successfully\")\n",
    "        print(f\"  - Blocks file: {blocks_file.name}\")\n",
    "        print(f\"  - Boundary file: {boundary_file.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading census data: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_census_data(census_data: dict) -> None:\n",
    "    \"\"\"Perform comprehensive analysis of census data.\"\"\"\n",
    "    \n",
    "    if not census_data:\n",
    "        print(\"No census data to analyze\")\n",
    "        return\n",
    "    \n",
    "    blocks = census_data['blocks']\n",
    "    boundary = census_data['boundary']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CENSUS DATA ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Geographic reprojection:\n",
    "\n",
    "    # Basic statistics\n",
    "\n",
    "    # EPSG:5070 designed for continental US\n",
    "    target_crs = \"EPSG:5070\"\n",
    "    blocks_projected = blocks.to_crs(target_crs)\n",
    "    boundary_projected = boundary.to_crs(target_crs)\n",
    "    \n",
    "    print(f\"\\nüìä BASIC STATISTICS:\")\n",
    "    print(f\"  -  Census data is reprojected to: {target_crs} (CONUS Albers)\")\n",
    "\n",
    "    print(f\"  - Number of census blocks: {len(blocks):,}\")\n",
    "    print(f\"  - Total boundary area: {boundary_projected.geometry.area.sum()/1e6:.2f} km¬≤\")\n",
    "    print(f\"  - Average block area: {blocks_projected.geometry.area.mean()/1e6:.4f} km¬≤\")\n",
    "    \n",
    "    # Demographic statistics (if available)\n",
    "    demographic_cols = [col for col in blocks.columns if any(x in col for x in ['POP20', 'HOUSING20'])]\n",
    "    if demographic_cols:\n",
    "        for col in demographic_cols:        \n",
    "            total = blocks[col].sum()\n",
    "            print(f\"  - Total {col}: {total:,}\")\n",
    "    \n",
    "    # Sanity checks\n",
    "    print(f\"\\nüîç SANITY CHECKS:\")\n",
    "    \n",
    "    # Check for empty geometries\n",
    "    empty_blocks = blocks.geometry.is_empty.sum()\n",
    "    print(f\"  - Empty geometries: {'‚úì' if empty_blocks == 0 else '‚úó'} ({empty_blocks} empty)\")\n",
    "    \n",
    "    # Check spatial coverage\n",
    "    blocks_within_boundary = blocks.geometry.within(boundary.geometry.iloc[0]).sum()\n",
    "    coverage_pct = (blocks_within_boundary / len(blocks)) * 100\n",
    "    print(f\"  - Blocks within boundary: {'‚úì' if coverage_pct > 95 else '‚úó'} ({coverage_pct:.1f}%)\")\n",
    "    \n",
    "   # Visualization\n",
    "    _, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Map visualization\n",
    "    boundary.plot(ax=ax, color='red', alpha=0.3, edgecolor='red', linewidth=2)\n",
    "    blocks.plot(ax=ax, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    ax.set_title('Census Blocks and Regional Boundary')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Execute census data analysis\n",
    "census_data = load_and_validate_census_data(REGION_PATH)\n",
    "analyze_census_data(census_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis on the coverage of the census blocks and the county as a subdivision, as not perfect overlap can be guaranteed.\n",
    "def analyze_spatial_coverage(blocks_projected: gpd.GeoDataFrame, boundary_projected: gpd.GeoDataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze and visualize spatial coverage of census blocks within boundary.\n",
    "    \n",
    "    This function performs comprehensive spatial coverage analysis including:\n",
    "    - Classification of blocks as inside, partial, or outside boundary\n",
    "    - Distance analysis for outside blocks\n",
    "    - Demographic impact assessment\n",
    "    - Detailed visualizations with problematic areas highlighted\n",
    "    \n",
    "    Args:\n",
    "        blocks_projected: GeoDataFrame of census blocks in projected CRS\n",
    "        boundary_projected: GeoDataFrame of boundary in projected CRS\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing coverage statistics and analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\nüó∫Ô∏è SPATIAL COVERAGE ANALYSIS:\")\n",
    "    \n",
    "    results = {\n",
    "        'within_count': 0,\n",
    "        'intersect_count': 0,\n",
    "        'outside_count': 0,\n",
    "        'partial_count': 0,\n",
    "        'coverage_within_pct': 0.0,\n",
    "        'coverage_intersect_pct': 0.0,\n",
    "        'outside_blocks': None,\n",
    "        'success': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Use projected coordinates for accurate spatial operations\n",
    "        blocks_within = blocks_projected.geometry.within(boundary_projected.geometry.iloc[0])\n",
    "        blocks_intersect = blocks_projected.geometry.intersects(boundary_projected.geometry.iloc[0])\n",
    "        \n",
    "        within_count = blocks_within.sum()\n",
    "        intersect_count = blocks_intersect.sum()\n",
    "        outside_count = len(blocks_projected) - intersect_count\n",
    "        \n",
    "        coverage_within_pct = (within_count / len(blocks_projected)) * 100\n",
    "        coverage_intersect_pct = (intersect_count / len(blocks_projected)) * 100\n",
    "        \n",
    "        # Update results dictionary\n",
    "        results.update({\n",
    "            'within_count': within_count,\n",
    "            'intersect_count': intersect_count,\n",
    "            'outside_count': outside_count,\n",
    "            'coverage_within_pct': coverage_within_pct,\n",
    "            'coverage_intersect_pct': coverage_intersect_pct\n",
    "        })\n",
    "        print(f\"  - Total blocks: {len(blocks_projected):,}\")\n",
    "        print(f\"  - Blocks completely within boundary: {within_count:,} ({coverage_within_pct:.1f}%)\")\n",
    "        print(f\"  - Blocks intersecting boundary: {intersect_count:,} ({coverage_intersect_pct:.1f}%)\")\n",
    "        print(f\"  - Blocks completely outside: {outside_count:,} ({(outside_count/len(blocks_projected)*100):.1f}%)\")\n",
    "        \n",
    "        # Identify different categories of blocks\n",
    "        blocks_inside = blocks_projected[blocks_within]\n",
    "        blocks_partial = blocks_projected[blocks_intersect & ~blocks_within]  # Intersect but not within\n",
    "        blocks_outside = blocks_projected[~blocks_intersect]  # Completely outside\n",
    "        \n",
    "        partial_count = len(blocks_partial)\n",
    "        results['partial_count'] = partial_count\n",
    "        results['outside_blocks'] = blocks_outside\n",
    "        \n",
    "        print(f\"  - Blocks partially overlapping: {partial_count:,}\")\n",
    "        \n",
    "        # Analyze the outside blocks\n",
    "        if len(blocks_outside) > 0:\n",
    "            print(f\"\\nüö® BLOCKS OUTSIDE BOUNDARY ANALYSIS:\")\n",
    "            \n",
    "            # Calculate distances from boundary for outside blocks\n",
    "            boundary_geom = boundary_projected.geometry.iloc[0]\n",
    "            distances = blocks_outside.geometry.centroid.distance(boundary_geom)\n",
    "            \n",
    "            print(f\"  - Distance range: {distances.min():.0f} - {distances.max():.0f} meters\")\n",
    "            print(f\"  - Mean distance: {distances.mean():.0f} meters\")\n",
    "            print(f\"  - Median distance: {distances.median():.0f} meters\")\n",
    "            \n",
    "            # Check if they have population/housing data\n",
    "            if 'POP20' in blocks_outside.columns:\n",
    "                outside_pop = blocks_outside['POP20'].sum()\n",
    "                outside_housing = blocks_outside['HOUSING20'].sum() if 'HOUSING20' in blocks_outside.columns else 0\n",
    "                print(f\"  - Population in outside blocks: {outside_pop:,}\")\n",
    "                print(f\"  - Housing units in outside blocks: {outside_housing:,}\")\n",
    "                \n",
    "                # Store in results\n",
    "                results['outside_population'] = outside_pop\n",
    "                results['outside_housing'] = outside_housing\n",
    "        \n",
    "        # Create detailed visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Map 1: Overview with all blocks\n",
    "        boundary_projected.plot(ax=axes[0], color='red', alpha=0.3, edgecolor='red', linewidth=2, label='Boundary')\n",
    "        \n",
    "        if len(blocks_inside) > 0:\n",
    "            blocks_inside.plot(ax=axes[0], color='green', alpha=0.7, edgecolor='darkgreen', \n",
    "                              linewidth=0.1, label=f'Inside ({len(blocks_inside):,})')\n",
    "        \n",
    "        if len(blocks_partial) > 0:\n",
    "            blocks_partial.plot(ax=axes[0], color='orange', alpha=0.7, edgecolor='darkorange', \n",
    "                               linewidth=0.1, label=f'Partial ({len(blocks_partial):,})')\n",
    "        \n",
    "        if len(blocks_outside) > 0:\n",
    "            blocks_outside.plot(ax=axes[0], color='red', alpha=0.7, edgecolor='darkred', \n",
    "                               linewidth=0.1, label=f'Outside ({len(blocks_outside):,})')\n",
    "        \n",
    "        axes[0].set_title('Census Blocks Spatial Coverage')\n",
    "        axes[0].legend()\n",
    "        axes[0].set_xlabel('Easting (m)')\n",
    "        axes[0].set_ylabel('Northing (m)')\n",
    "        \n",
    "        # Map 2: Focus on problematic areas (if any)\n",
    "        if len(blocks_outside) > 0 or len(blocks_partial) > 0:\n",
    "            # Calculate bounds that include boundary and problematic blocks\n",
    "            all_problematic = pd.concat([blocks_partial, blocks_outside]) if len(blocks_partial) > 0 and len(blocks_outside) > 0 else (blocks_partial if len(blocks_partial) > 0 else blocks_outside)\n",
    "            \n",
    "            boundary_projected.plot(ax=axes[1], color='red', alpha=0.3, edgecolor='red', linewidth=2, label='Boundary')\n",
    "            \n",
    "            if len(blocks_partial) > 0:\n",
    "                blocks_partial.plot(ax=axes[1], color='orange', alpha=0.8, edgecolor='darkorange', \n",
    "                                   linewidth=0.5, label=f'Partial overlap ({len(blocks_partial):,})')\n",
    "            \n",
    "            if len(blocks_outside) > 0:\n",
    "                blocks_outside.plot(ax=axes[1], color='red', alpha=0.8, edgecolor='darkred', \n",
    "                                   linewidth=0.5, label=f'Outside ({len(blocks_outside):,})')\n",
    "            \n",
    "            # Add some nearby inside blocks for context\n",
    "            if len(blocks_inside) > 0:\n",
    "                sample_inside = blocks_inside.sample(min(100, len(blocks_inside)))\n",
    "                sample_inside.plot(ax=axes[1], color='lightgreen', alpha=0.6, edgecolor='green', \n",
    "                                  linewidth=0.1, label='Inside (sample)')\n",
    "            \n",
    "            axes[1].set_title('Focus: Problematic Blocks')\n",
    "            axes[1].legend()\n",
    "            axes[1].set_xlabel('Easting (m)')\n",
    "            axes[1].set_ylabel('Northing (m)')\n",
    "            \n",
    "            # Set bounds to focus on problematic areas\n",
    "            bounds = all_problematic.total_bounds\n",
    "            buffer = max(bounds[2] - bounds[0], bounds[3] - bounds[1]) * 0.1  # 10% buffer\n",
    "            axes[1].set_xlim(bounds[0] - buffer, bounds[2] + buffer)\n",
    "            axes[1].set_ylim(bounds[1] - buffer, bounds[3] + buffer)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No problematic blocks found!\\nAll blocks are within boundary.', \n",
    "                        transform=axes[1].transAxes, ha='center', va='center', fontsize=14,\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "            axes[1].set_title('Spatial Coverage: EXCELLENT')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional analysis for problematic blocks\n",
    "        if len(blocks_outside) > 0:\n",
    "            print(f\"\\nüîç DETAILED ANALYSIS OF OUTSIDE BLOCKS:\")\n",
    "            \n",
    "            # Show some example outside blocks\n",
    "            print(f\"Sample of blocks outside boundary (first 5):\")\n",
    "            sample_outside = blocks_outside.head()\n",
    "            \n",
    "            boundary_geom = boundary_projected.geometry.iloc[0]\n",
    "            \n",
    "            for idx, row in sample_outside.iterrows():\n",
    "                geoid = row.get('GEOID20', idx)\n",
    "                pop = row.get('POP20', 'N/A')\n",
    "                housing = row.get('HOUSING20', 'N/A')\n",
    "                area = row.geometry.area / 1e6  # km¬≤\n",
    "                \n",
    "                # Distance from boundary\n",
    "                dist = row.geometry.centroid.distance(boundary_geom)\n",
    "                \n",
    "                print(f\"  - Block {geoid}: Pop={pop}, Housing={housing}, Area={area:.4f}km¬≤, Distance={dist:.0f}m\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nüí° SPATIAL COVERAGE RECOMMENDATIONS:\")\n",
    "        \n",
    "        if coverage_intersect_pct >= 99:\n",
    "            print(f\"  ‚úÖ Excellent spatial coverage - minor edge effects only\")\n",
    "            results['recommendation'] = 'excellent'\n",
    "        elif coverage_intersect_pct >= 95:\n",
    "            print(f\"  ‚ö†Ô∏è Good coverage - some blocks extend beyond boundary\")\n",
    "            print(f\"     Consider: boundary definition or block selection criteria\")\n",
    "            results['recommendation'] = 'good'\n",
    "        else:\n",
    "            print(f\"  ‚ùå Poor coverage - significant spatial mismatch\")\n",
    "            print(f\"     Action needed: check boundary definition and block selection\")\n",
    "            results['recommendation'] = 'poor'\n",
    "            \n",
    "        if len(blocks_outside) > 0:\n",
    "            outside_with_pop = blocks_outside[blocks_outside.get('POP20', 0) > 0] if 'POP20' in blocks_outside.columns else []\n",
    "            if len(outside_with_pop) > 0:\n",
    "                print(f\"  ‚ö†Ô∏è {len(outside_with_pop)} outside blocks have population - data may be lost\")\n",
    "                results['data_loss_risk'] = len(outside_with_pop)\n",
    "        \n",
    "        results['success'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error in spatial coverage analysis: {e}\")\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# In your analyze_census_data function, replace the spatial coverage code with:\n",
    "blocks_projected = census_data['blocks'].to_crs(epsg=5070)\n",
    "boundary_projected = census_data['boundary'].to_crs(epsg=5070)\n",
    "coverage_results = analyze_spatial_coverage(blocks_projected, boundary_projected)\n",
    "\n",
    "# You can also use the results for further analysis:\n",
    "if coverage_results['success']:\n",
    "    print(f\"Coverage analysis completed with {coverage_results['coverage_intersect_pct']:.1f}% intersection\")\n",
    "else:\n",
    "    print(f\"Coverage analysis failed: {coverage_results.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample block of your choice\n",
    "\n",
    "def plot_block_with_population_housing_and_buildings(\n",
    "    blocks_gdf: gpd.GeoDataFrame,\n",
    "    target_geoid20: str,\n",
    "    title: str = \"Census Block with Population/Housing Data\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots a specific census block, its population and housing data,\n",
    "    and an OpenStreetMap basemap which includes building outlines.\n",
    "\n",
    "    Args:\n",
    "        blocks_gdf (gpd.GeoDataFrame): GeoDataFrame containing census blocks.\n",
    "                                       Must include 'GEOID20', 'POP20',\n",
    "                                       and 'HOUSING20' columns.\n",
    "        target_geoid20 (str): The GEOID20 of the block to plot.\n",
    "        title (str, optional): The title for the plot.\n",
    "                               Defaults to \"Census Block with Population/Housing Data\".\n",
    "    \"\"\"\n",
    "    # Ensure GEOID20 column is string type for matching\n",
    "    if 'GEOID20' not in blocks_gdf.columns:\n",
    "        print(\"Error: 'GEOID20' column not found in GeoDataFrame.\")\n",
    "        return\n",
    "    \n",
    "    blocks_gdf['GEOID20'] = blocks_gdf['GEOID20'].astype(str)\n",
    "    target_geoid20 = str(target_geoid20)\n",
    "\n",
    "    # Filter for the target block\n",
    "    target_block = blocks_gdf[blocks_gdf['GEOID20'] == target_geoid20]\n",
    "\n",
    "    if target_block.empty:\n",
    "        print(f\"Block with GEOID20 '{target_geoid20}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Ensure POP20 and HOUSING20 columns exist\n",
    "    if 'POP20' not in target_block.columns or 'HOUSING20' not in target_block.columns:\n",
    "        print(\"Error: 'POP20' or 'HOUSING20' columns not found for the selected block.\")\n",
    "        return\n",
    "\n",
    "    # Get population and housing data\n",
    "    population = target_block['POP20'].iloc[0]\n",
    "    housing_units = target_block['HOUSING20'].iloc[0]\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "\n",
    "    # Plot the target block\n",
    "    # Use a contrasting color and some transparency to see basemap\n",
    "    target_block.to_crs(epsg=3857).plot(\n",
    "        ax=ax,\n",
    "        alpha=0.5,\n",
    "        edgecolor='red',\n",
    "        facecolor='yellow',\n",
    "        linewidth=2,\n",
    "        label=f\"Block {target_geoid20}\"\n",
    "    )\n",
    "\n",
    "    # Add basemap from OpenStreetMap (often shows buildings)\n",
    "    # Adjust zoom level if needed, or let contextily auto-determine\n",
    "    try:\n",
    "        ctx.add_basemap(\n",
    "            ax,\n",
    "            crs=target_block.to_crs(epsg=3857).crs.to_string(),\n",
    "            source=ctx.providers.OpenStreetMap.Mapnik, # This source usually has building footprints\n",
    "            zoom='auto' # Let contextily determine zoom from bounds, or set manually e.g. 18\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not add basemap: {e}\")\n",
    "\n",
    "\n",
    "    # Add text annotation for population and housing\n",
    "    # Get centroid of the block for placing the text\n",
    "    # Ensure the geometry is not empty and is valid\n",
    "    if not target_block.geometry.iloc[0].is_empty and target_block.geometry.iloc[0].is_valid:\n",
    "        centroid = target_block.geometry.iloc[0].centroid\n",
    "        \n",
    "        # Project centroid to the plot's CRS (EPSG:3857) for correct annotation placement\n",
    "        centroid_map_crs = gpd.GeoSeries([centroid], crs=target_block.crs).to_crs(epsg=3857).iloc[0]\n",
    "\n",
    "        ax.annotate(\n",
    "            text=f\"GEOID: {target_geoid20}\\nPopulation (POP20): {population}\\nHousing Units (HOUSING20): {housing_units}\",\n",
    "            xy=(centroid_map_crs.x, centroid_map_crs.y),\n",
    "            xytext=(5, 5),  # Offset text slightly\n",
    "            textcoords=\"offset points\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.7),\n",
    "            fontsize=9\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Warning: Geometry for block {target_geoid20} is empty or invalid. Cannot place annotation.\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_block_with_population_housing_and_buildings(\n",
    "    blocks_gdf=census_data['blocks'],\n",
    "    target_geoid20=random.sample(census_data['blocks']['GEOID20'].tolist(), 1)[0],\n",
    "    title=f\"Details for Block\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NREL Data Analysis\n",
    "\n",
    "This section evaluates the NREL building characteristics data from Step 2, including:\n",
    "- Building vintage distributions\n",
    "- Energy characteristics\n",
    "- Spatial coverage within the target region\n",
    "\n",
    "**Key Outputs to Validate:**\n",
    "- NREL parquet file with building data\n",
    "- Vintage distribution statistics\n",
    "- Spatial alignment with census boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_nrel_data(nrel_path) -> None:\n",
    "    \"\"\"Perform comprehensive analysis of NREL data.\"\"\"\n",
    "    \n",
    "    # Check for any .parquet files in the NREL directory and select first and read to dataframe: \n",
    "    parquet_files = list(nrel_path.glob('*.parquet'))\n",
    "    if not parquet_files:\n",
    "        print(\"No .parquet files found in the NREL directory\")\n",
    "        return\n",
    "    \n",
    "    data = pd.read_parquet(parquet_files[0])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"NREL DATA ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä BASIC STATISTICS:\")\n",
    "    print(f\"  - Total NREL records: {len(data):,}\")\n",
    "    print(f\"  - Columns available: {len(data.columns)}\")\n",
    "    \n",
    "    # Coverage check\n",
    "    if len(data) > 100:\n",
    "        print(f\"  ‚úÖ Dataset has good coverage (>100 records)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Small dataset - (<100 records)\")\n",
    "    \n",
    "    # Vintage Distribution Analysis\n",
    "    print(f\"\\nüè† VINTAGE DISTRIBUTION ANALYSIS:\")\n",
    "    \n",
    "    if 'in.vintage' in data.columns:\n",
    "        vintage_counts = data['in.vintage'].value_counts().sort_index()\n",
    "        total_records = len(data)\n",
    "        \n",
    "        print(f\"  üìà Vintage Distribution:\")\n",
    "        print(f\"     {'Vintage':<12} {'Count':<8} {'Percentage':<10}\")\n",
    "        print(f\"     {'-'*32}\")\n",
    "        \n",
    "        for vintage, count in vintage_counts.items():\n",
    "            percentage = (count / total_records) * 100\n",
    "            print(f\"     {vintage:<12} {count:<8} {percentage:>8.2f}%\")\n",
    "        \n",
    "        # Create vintage distribution visualization - pie chart only\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        vintage_counts.plot(kind='pie', ax=ax, autopct='%1.1f%%', startangle=90)\n",
    "        ax.set_title('NREL Vintage Distribution')\n",
    "        ax.set_ylabel('')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"  ‚ùå 'in.vintage' column not found\")\n",
    "    \n",
    "    # Building Type Distribution Analysis\n",
    "    print(f\"\\nüè¢ BUILDING TYPE DISTRIBUTION ANALYSIS:\")\n",
    "    \n",
    "    building_type_col = 'in.geometry_building_type_acs'\n",
    "    if building_type_col in data.columns:\n",
    "        \n",
    "        def map_building_type(bt: str) -> str:\n",
    "            \"\"\"Map NREL building types to simplified codes.\"\"\"\n",
    "            if pd.isna(bt):\n",
    "                return \"Unknown\"\n",
    "            \n",
    "            bt_str = str(bt).strip()\n",
    "            \n",
    "            if bt_str == \"Single-Family Detached\":\n",
    "                return \"SFH\"\n",
    "            elif bt_str == \"Mobile Home\":\n",
    "                return \"SFH\"\n",
    "            elif bt_str == \"Single-Family Attached\":\n",
    "                return \"TH\"\n",
    "            elif bt_str in [\"2 Unit\", \"3 or 4 Unit\", \"5 to 9 Unit\"]:\n",
    "                return \"MFH\"\n",
    "            elif bt_str in [\"10 to 19 Unit\", \"20 to 49 Unit\", \"50 or more Unit\"]:\n",
    "                return \"AB\"\n",
    "            else:\n",
    "                return \"Other\"\n",
    "        \n",
    "        # Raw building types\n",
    "        raw_building_types = data[building_type_col].value_counts()\n",
    "        total_records = len(data)\n",
    "        \n",
    "        print(f\"  üìà Raw Building Type Distribution:\")\n",
    "        print(f\"     {'Building Type':<25} {'Count':<8} {'Percentage':<10}\")\n",
    "        print(f\"     {'-'*45}\")\n",
    "        \n",
    "        for btype, count in raw_building_types.items():\n",
    "            percentage = (count / total_records) * 100\n",
    "            print(f\"     {str(btype):<25} {count:<8} {percentage:>8.2f}%\")\n",
    "        \n",
    "        # Simplified building types\n",
    "        data_temp = data.copy()\n",
    "        data_temp['simplified_type'] = data_temp[building_type_col].apply(map_building_type)\n",
    "        simplified_types = data_temp['simplified_type'].value_counts()\n",
    "        \n",
    "        print(f\"\\n  üìä Simplified Building Type Distribution:\")\n",
    "        print(f\"     {'Type':<8} {'Description':<20} {'Count':<8} {'Percentage':<10}\")\n",
    "        print(f\"     {'-'*50}\")\n",
    "        \n",
    "        type_descriptions = {\n",
    "            'SFH': 'Single Family Home',\n",
    "            'TH': 'Townhouse',\n",
    "            'MFH': 'Multi-Family (2-9)',\n",
    "            'AB': 'Apartment (10+)',\n",
    "            'Other': 'Other/Unknown'\n",
    "        }\n",
    "        \n",
    "        for stype, count in simplified_types.items():\n",
    "            percentage = (count / total_records) * 100\n",
    "            desc = type_descriptions.get(stype, 'Unknown')\n",
    "            print(f\"     {stype:<8} {desc:<20} {count:<8} {percentage:>8.2f}%\")\n",
    "        \n",
    "        # Building type visualization - pie charts only\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Raw types pie chart\n",
    "        raw_building_types.plot(kind='pie', ax=axes[0], autopct='%1.1f%%', startangle=90)\n",
    "        axes[0].set_title('Raw Building Types')\n",
    "        axes[0].set_ylabel('')\n",
    "        \n",
    "        # Simplified types pie chart\n",
    "        simplified_types.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
    "        axes[1].set_title('Simplified Building Types')\n",
    "        axes[1].set_ylabel('')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"  ‚ùå '{building_type_col}' column not found\")\n",
    "    \n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = data.duplicated().sum()\n",
    "    print(f\"  - Duplicate records: {'‚úì' if duplicates == 0 else '‚ö†Ô∏è'} ({duplicates} duplicates)\")\n",
    "    \n",
    "\n",
    "nrel_path = Path(REGION_PATH) / \"NREL\"\n",
    "analyze_nrel_data(nrel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OSM Data Analysis\n",
    "\n",
    "This section evaluates the OpenStreetMap data extraction from Step 3, including:\n",
    "- Infrastructure features (buildings, pois, landuse, powerfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_osm_data(region_path: Path) -> dict:\n",
    "    \"\"\"Load and validate OSM data outputs.\"\"\"\n",
    "    osm_path = region_path / \"OSM\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Look for common OSM output files\n",
    "        osm_files = list(osm_path.glob(\"*.geojson\")) + list(osm_path.glob(\"*.gpkg\"))\n",
    "        \n",
    "        if not osm_files:\n",
    "            print(\"‚úó No OSM output files found\")\n",
    "            return {}\n",
    "        \n",
    "        results['files'] = osm_files\n",
    "        results['data'] = {}\n",
    "        \n",
    "        # Load each OSM file\n",
    "        for file_path in osm_files:\n",
    "            try:\n",
    "                data = gpd.read_file(file_path)\n",
    "                file_key = file_path.stem\n",
    "                results['data'][file_key] = data\n",
    "                print(f\"‚úì Loaded {file_key}: {len(data):,} features\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error loading {file_path.name}: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error accessing OSM data: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_osm_data(osm_data: dict, census_data: dict) -> None:\n",
    "    \"\"\"Perform comprehensive analysis of OSM data.\"\"\"\n",
    "    \n",
    "    if not osm_data or not osm_data.get('data'):\n",
    "        print(\"No OSM data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"OSM DATA ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_features = sum(len(gdf) for gdf in osm_data['data'].values())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä BASIC STATISTICS:\")\n",
    "    print(f\"  - Total OSM datasets: {len(osm_data['data'])}\")\n",
    "    print(f\"  - Total features: {total_features:,}\")\n",
    "    \n",
    "    \n",
    "    # Geometry type analysis\n",
    "    print(f\"\\nüî∫ GEOMETRY ANALYSIS:\")\n",
    "    for name, gdf in osm_data['data'].items():\n",
    "        if len(gdf) > 0:\n",
    "            geom_types = gdf.geometry.geom_type.value_counts()\n",
    "            print(f\"  - {name}: {geom_types.to_dict()}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Feature count by dataset\n",
    "    dataset_counts = {name: len(gdf) for name, gdf in osm_data['data'].items()}\n",
    "    axes[0,0].bar(dataset_counts.keys(), dataset_counts.values())\n",
    "    axes[0,0].set_title('OSM Features by Dataset')\n",
    "    axes[0,0].set_ylabel('Feature Count')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Geometry type distribution\n",
    "    all_geom_types = {}\n",
    "    for gdf in osm_data['data'].values():\n",
    "        if len(gdf) > 0:\n",
    "            for geom_type, count in gdf.geometry.geom_type.value_counts().items():\n",
    "                all_geom_types[geom_type] = all_geom_types.get(geom_type, 0) + count\n",
    "    \n",
    "    if all_geom_types:\n",
    "        axes[0,1].pie(all_geom_types.values(), labels=all_geom_types.keys(), autopct='%1.1f%%')\n",
    "        axes[0,1].set_title('Geometry Type Distribution')\n",
    "    \n",
    "    # Spatial plot (if boundary available)\n",
    "    if census_data and 'boundary' in census_data:\n",
    "        boundary = census_data['boundary']\n",
    "        boundary.plot(ax=axes[1,0], color='red', alpha=0.3, edgecolor='red', linewidth=2)\n",
    "        \n",
    "        # Plot a sample of OSM features\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(osm_data['data'])))\n",
    "        for (name, gdf), color in zip(osm_data['data'].items(), colors):\n",
    "            if len(gdf) > 0:\n",
    "                sample_size = min(1000, len(gdf))  # Sample for performance\n",
    "                gdf.sample(sample_size).plot(ax=axes[1,0], color=color, alpha=0.6, markersize=1, label=name)\n",
    "        \n",
    "        axes[1,0].set_title('OSM Features Spatial Distribution')\n",
    "        axes[1,0].legend()\n",
    "    \n",
    "    # Missing values analysis\n",
    "    all_missing = {}\n",
    "    for name, gdf in osm_data['data'].items():\n",
    "        if len(gdf) > 0:\n",
    "            missing = gdf.isnull().sum().sum()\n",
    "            all_missing[name] = missing\n",
    "    \n",
    "    if all_missing:\n",
    "        axes[1,1].bar(all_missing.keys(), all_missing.values())\n",
    "        axes[1,1].set_title('Missing Values by Dataset')\n",
    "        axes[1,1].set_ylabel('Missing Value Count')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Execute OSM data analysis\n",
    "osm_data = load_and_validate_osm_data(REGION_PATH)\n",
    "analyze_osm_data(osm_data, census_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced OSM Data Analysis with ydata_profiling\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "import warnings\n",
    "import contextily as ctx\n",
    "from ydata_profiling import ProfileReport\n",
    "import folium\n",
    "from folium import plugins\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_and_validate_osm_data(region_path: Path) -> dict:\n",
    "    \"\"\"Load and validate OSM data outputs.\"\"\"\n",
    "    osm_path = region_path / \"OSM\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Look for common OSM output files\n",
    "        osm_files = list(osm_path.glob(\"*.geojson\")) + list(osm_path.glob(\"*.gpkg\"))\n",
    "        \n",
    "        if not osm_files:\n",
    "            print(\"‚úó No OSM output files found\")\n",
    "            return {}\n",
    "        \n",
    "        results['files'] = osm_files\n",
    "        results['data'] = {}\n",
    "        \n",
    "        # Load each OSM file\n",
    "        for file_path in osm_files:\n",
    "            try:\n",
    "                data = gpd.read_file(file_path)\n",
    "                file_key = file_path.stem\n",
    "                results['data'][file_key] = data\n",
    "                print(f\"‚úì Loaded {file_key}: {len(data):,} features\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error loading {file_path.name}: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error accessing OSM data: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_osm_data(osm_data: dict, census_data=None):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of OSM datasets: power, buildings, pois, landuse\n",
    "    \"\"\"\n",
    "    \n",
    "    if not osm_data or 'data' not in osm_data:\n",
    "        print(\"‚úó No OSM data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Extract datasets from osm_data structure\n",
    "    datasets = osm_data['data']\n",
    "    \n",
    "    # Create output directory for reports\n",
    "    if osm_data.get('files'):\n",
    "        base_path = osm_data['files'][0].parent\n",
    "        reports_dir = base_path / \"analysis_reports\"\n",
    "    else:\n",
    "        reports_dir = Path(\"./analysis_reports\")\n",
    "    \n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Map dataset keys to standard names\n",
    "    dataset_mapping = {}\n",
    "    for key, gdf in datasets.items():\n",
    "        key_lower = key.lower()\n",
    "        if 'power' in key_lower:\n",
    "            dataset_mapping['power'] = gdf\n",
    "        elif 'building' in key_lower:\n",
    "            dataset_mapping['buildings'] = gdf\n",
    "        elif 'poi' in key_lower or 'amenity' in key_lower:\n",
    "            dataset_mapping['pois'] = gdf\n",
    "        elif 'landuse' in key_lower or 'land_use' in key_lower:\n",
    "            dataset_mapping['landuse'] = gdf\n",
    "    \n",
    "    # Use mapped datasets\n",
    "    datasets = dataset_mapping\n",
    "    \n",
    "    # 1. POWER ANALYSIS\n",
    "    if 'power' in datasets:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"POWER INFRASTRUCTURE ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        power_gdf = datasets['power']\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"\\nOverall Power Infrastructure Count: {len(power_gdf):,}\")\n",
    "        \n",
    "        # Analyze power types\n",
    "        if 'power' in power_gdf.columns:\n",
    "            power_types = power_gdf['power'].value_counts()\n",
    "            print(f\"\\nPower Type Distribution:\")\n",
    "            for ptype, count in power_types.items():\n",
    "                print(f\"  {ptype}: {count:,} ({count/len(power_gdf)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\nNo 'power' column found for type classification\")\n",
    "            power_types = pd.Series({'unknown': len(power_gdf)})\n",
    "        \n",
    "        # Create visualizations for power\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Power Infrastructure Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Power type distribution bar chart\n",
    "        axes[0,0].bar(power_types.index, power_types.values, color='skyblue', edgecolor='navy')\n",
    "        axes[0,0].set_title('Power Infrastructure Count by Type')\n",
    "        axes[0,0].set_xlabel('Power Type')\n",
    "        axes[0,0].set_ylabel('Count')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Power type distribution pie chart\n",
    "        axes[0,1].pie(power_types.values, labels=power_types.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[0,1].set_title('Power Infrastructure Distribution')\n",
    "        \n",
    "        # Geographic distribution of power infrastructure\n",
    "        power_gdf.plot(ax=axes[1,0], color='red', markersize=20, alpha=0.7)\n",
    "        axes[1,0].set_title('Geographic Distribution of Power Infrastructure')\n",
    "        axes[1,0].set_xlabel('Longitude')\n",
    "        axes[1,0].set_ylabel('Latitude')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Substation analysis if available\n",
    "        if 'substation' in power_gdf.columns:\n",
    "            substation_types = power_gdf['substation'].value_counts().dropna()\n",
    "            if len(substation_types) > 0:\n",
    "                axes[1,1].bar(substation_types.index, substation_types.values, color='orange', edgecolor='darkred')\n",
    "                axes[1,1].set_title('Substation Type Distribution')\n",
    "                axes[1,1].set_xlabel('Substation Type')\n",
    "                axes[1,1].set_ylabel('Count')\n",
    "                axes[1,1].tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                print(f\"\\nSubstation Type Distribution:\")\n",
    "                for stype, count in substation_types.items():\n",
    "                    print(f\"  {stype}: {count:,}\")\n",
    "            else:\n",
    "                axes[1,1].text(0.5, 0.5, 'No substation types\\nwith data', \n",
    "                              ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "                axes[1,1].set_title('Substation Classification (Empty)')\n",
    "        else:\n",
    "            axes[1,1].text(0.5, 0.5, 'No substation\\nclassification available', \n",
    "                          ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "            axes[1,1].set_title('Substation Classification (N/A)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(reports_dir / 'power_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate ydata_profiling report for power\n",
    "        power_df = power_gdf.drop(columns=['geometry']).copy()\n",
    "        power_profile = ProfileReport(power_df, title=\"Power Infrastructure Profile Report\", explorative=True)\n",
    "        power_profile.to_file(reports_dir / \"power_profile_report.html\")\n",
    "        print(f\"‚úì Power profile report saved to: {reports_dir / 'power_profile_report.html'}\")\n",
    "    \n",
    "\n",
    "        \n",
    "    # 2. BUILDINGS ANALYSIS\n",
    "    if 'buildings' in datasets:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BUILDINGS ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        buildings_gdf = datasets['buildings']\n",
    "        print(f\"\\nTotal Buildings Count: {len(buildings_gdf):,}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        fig.suptitle('Buildings Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Building count\n",
    "        axes[0].bar(['Total Buildings'], [len(buildings_gdf)], color='lightblue', edgecolor='navy')\n",
    "        axes[0].set_title('Total Building Count')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        for i, v in enumerate([len(buildings_gdf)]):\n",
    "            axes[0].text(i, v + v*0.01, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Building types analysis\n",
    "        if 'building' in buildings_gdf.columns:\n",
    "            building_types = buildings_gdf['building'].value_counts().head(10)\n",
    "            axes[1].barh(building_types.index, building_types.values, color='lightgreen', edgecolor='darkgreen')\n",
    "            axes[1].set_title('Top 10 Building Types')\n",
    "            axes[1].set_xlabel('Count')\n",
    "            \n",
    "            print(f\"\\nBuilding Type Distribution (Top 10):\")\n",
    "            for btype, count in building_types.items():\n",
    "                print(f\"  {btype}: {count:,}\")\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No building type\\ninformation available', \n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "            axes[1].set_title('Building Types (N/A)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(reports_dir / 'buildings_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "                \n",
    "        # Generate ydata_profiling report for buildings\n",
    "        buildings_df = buildings_gdf.drop(columns=['geometry']).copy()\n",
    "        buildings_profile = ProfileReport(buildings_df, title=\"Buildings Profile Report\", explorative=True)\n",
    "        buildings_profile.to_file(reports_dir / \"buildings_profile_report.html\")\n",
    "        print(f\"‚úì Buildings profile report saved to: {reports_dir / 'buildings_profile_report.html'}\")\n",
    "    \n",
    "    if 'pois' in datasets:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"POINTS OF INTEREST ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        pois_gdf = datasets['pois']\n",
    "        print(f\"\\nTotal POIs Count: {len(pois_gdf):,}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle('Points of Interest Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # POI count\n",
    "        axes[0].bar(['Total POIs'], [len(pois_gdf)], color='lightcoral', edgecolor='darkred')\n",
    "        axes[0].set_title('Total POI Count')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        for i, v in enumerate([len(pois_gdf)]):\n",
    "            axes[0].text(i, v + v*0.01, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # POI amenity types\n",
    "        if 'amenity' in pois_gdf.columns:\n",
    "            amenity_types = pois_gdf['amenity'].value_counts().head(10)\n",
    "            axes[1].barh(amenity_types.index, amenity_types.values, color='lightpink', edgecolor='darkred')\n",
    "            axes[1].set_title('Top 10 Amenity Types')\n",
    "            axes[1].set_xlabel('Count')\n",
    "            \n",
    "            print(f\"\\nAmenity Type Distribution (Top 10):\")\n",
    "            for atype, count in amenity_types.items():\n",
    "                print(f\"  {atype}: {count:,}\")\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No amenity type\\ninformation available', \n",
    "                        ha='center', va='center', transform=axes[1].transAxes)\n",
    "            axes[1].set_title('Amenity Types (N/A)')\n",
    "        \n",
    "        # Geographic distribution\n",
    "        pois_sample = pois_gdf.sample(min(1000, len(pois_gdf))) if len(pois_gdf) > 1000 else pois_gdf\n",
    "        pois_sample.plot(ax=axes[2], color='purple', alpha=0.6, markersize=8)\n",
    "        axes[2].set_title('Geographic Distribution of POIs')\n",
    "        axes[2].set_xlabel('Longitude')\n",
    "        axes[2].set_ylabel('Latitude')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(reports_dir / 'pois_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate ydata_profiling report for POIs\n",
    "        pois_df = pois_gdf.drop(columns=['geometry']).copy()\n",
    "        pois_profile = ProfileReport(pois_df, title=\"POIs Profile Report\", explorative=True)\n",
    "        pois_profile.to_file(reports_dir / \"pois_profile_report.html\")\n",
    "        print(f\"‚úì POIs profile report saved to: {reports_dir / 'pois_profile_report.html'}\")\n",
    "    \n",
    "    # 4. LANDUSE ANALYSIS\n",
    "    if 'landuse' in datasets:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"LANDUSE ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        landuse_gdf = datasets['landuse']\n",
    "        print(f\"\\nTotal Landuse Areas Count: {len(landuse_gdf):,}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle('Landuse Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Landuse count\n",
    "        axes[0].bar(['Total Landuse Areas'], [len(landuse_gdf)], color='lightgreen', edgecolor='darkgreen')\n",
    "        axes[0].set_title('Total Landuse Count')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        for i, v in enumerate([len(landuse_gdf)]):\n",
    "            axes[0].text(i, v + v*0.01, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Landuse types\n",
    "        if 'landuse' in landuse_gdf.columns:\n",
    "            landuse_types = landuse_gdf['landuse'].value_counts().head(10)\n",
    "            axes[1].barh(landuse_types.index, landuse_types.values, color='lightblue', edgecolor='darkblue')\n",
    "            axes[1].set_title('Top 10 Landuse Types')\n",
    "            axes[1].set_xlabel('Count')\n",
    "            \n",
    "            print(f\"\\nLanduse Type Distribution (Top 10):\")\n",
    "            for ltype, count in landuse_types.items():\n",
    "                print(f\"  {ltype}: {count:,}\")\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No landuse type\\ninformation available', \n",
    "                          ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "            axes[1].set_title('Landuse Types (N/A)')\n",
    "        \n",
    "        # Geographic distribution\n",
    "        landuse_sample = landuse_gdf.sample(min(500, len(landuse_gdf))) if len(landuse_gdf) > 500 else landuse_gdf\n",
    "        landuse_sample.plot(ax=axes[2], color='green', alpha=0.5, edgecolor='darkgreen')\n",
    "        axes[2].set_title('Geographic Distribution of Landuse')\n",
    "        axes[2].set_xlabel('Longitude')\n",
    "        axes[2].set_ylabel('Latitude')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(reports_dir / 'landuse_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate ydata_profiling report for landuse\n",
    "        landuse_df = landuse_gdf.drop(columns=['geometry']).copy()\n",
    "        landuse_profile = ProfileReport(landuse_df, title=\"Landuse Profile Report\", explorative=True)\n",
    "        landuse_profile.to_file(reports_dir / \"landuse_profile_report.html\")\n",
    "        print(f\"‚úì Landuse profile report saved to: {reports_dir / 'landuse_profile_report.html'}\")\n",
    "    \n",
    "    # 5. COMBINED OVERVIEW\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COMBINED OVERVIEW\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create summary plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    dataset_counts = {name: len(gdf) for name, gdf in datasets.items()}\n",
    "    colors = ['red', 'blue', 'purple', 'green']\n",
    "    \n",
    "    if dataset_counts:\n",
    "        bars = ax.bar(dataset_counts.keys(), dataset_counts.values(), \n",
    "                      color=colors[:len(dataset_counts)], alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, dataset_counts.values()):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax.set_title('OSM Dataset Feature Counts Comparison', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel('Number of Features')\n",
    "        ax.set_xlabel('Dataset Type')\n",
    "        ax.set_yscale('log')  # Log scale for better comparison\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(reports_dir / 'combined_overview.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"No datasets found for analysis\")\n",
    "    \n",
    "    print(f\"\\n‚úì All analysis reports saved to: {reports_dir}\")\n",
    "    return datasets, reports_dir\n",
    "\n",
    "# Execute OSM data analysis\n",
    "osm_data = load_and_validate_osm_data(REGION_PATH)\n",
    "datasets, reports_dir = analyze_osm_data(osm_data, census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Data Analysis\n",
    "\n",
    "This section evaluates the building data processing from Steps 3.5 and 4, including:\n",
    "- Building classification heuristics\n",
    "\n",
    "**Key Outputs to Validate:**\n",
    "- Building classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_building_shapefiles(region_path: Path) -> dict:\n",
    "    \"\"\"Load residential and non-residential building shapefiles from SHP folder.\"\"\"\n",
    "    shp_path = region_path / \"BUILDINGS_OUTPUT\" / \"SHP\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Look for residential and non-residential shapefiles\n",
    "        print(shp_path)\n",
    "\n",
    "        shp_files = list(shp_path.glob(\"*.shp\"))\n",
    "        print(shp_files)\n",
    "        \n",
    "        for shp_file in shp_files:\n",
    "            filename = shp_file.stem.lower()\n",
    "            \n",
    "            if 'residential' in filename and 'non' not in filename:\n",
    "                try:\n",
    "                    results['residential'] = gpd.read_file(shp_file)\n",
    "                    print(f\"‚úì Loaded residential buildings: {len(results['residential']):,} buildings\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚úó Error loading residential shapefile: {e}\")\n",
    "                    \n",
    "            elif 'non_residential' in filename or 'nonresidential' in filename:\n",
    "                try:\n",
    "                    results['non_residential'] = gpd.read_file(shp_file)\n",
    "                    print(f\"‚úì Loaded non-residential buildings: {len(results['non_residential']):,} buildings\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚úó Error loading non-residential shapefile: {e}\")\n",
    "        \n",
    "        if not results:\n",
    "            print(\"‚úó No building shapefiles found in SHP folder\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error accessing SHP folder: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_building_distributions(buildings_gdf: gpd.GeoDataFrame, building_type: str, columns_to_analyze: list) -> None:\n",
    "    \"\"\"\n",
    "    Reusable function to analyze building feature distributions.\n",
    "    \n",
    "    Args:\n",
    "        buildings_gdf: GeoDataFrame with building data\n",
    "        building_type: Type description (e.g., \"Residential\", \"Non-Residential\")\n",
    "        columns_to_analyze: List of column names to analyze\n",
    "    \"\"\"\n",
    "    if buildings_gdf is None or len(buildings_gdf) == 0:\n",
    "        print(f\"No {building_type.lower()} building data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{building_type.upper()} BUILDINGS DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä BASIC STATISTICS:\")\n",
    "    print(f\"  - Total {building_type.lower()} buildings: {len(buildings_gdf):,}\")\n",
    "    print(f\"  - Available columns: {list(buildings_gdf.columns)}\")\n",
    "    \n",
    "    # Check which columns are available\n",
    "    available_columns = [col for col in columns_to_analyze if col in buildings_gdf.columns]\n",
    "    missing_columns = [col for col in columns_to_analyze if col not in buildings_gdf.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"  ‚ö†Ô∏è Missing columns: {missing_columns}\")\n",
    "    \n",
    "    if not available_columns:\n",
    "        print(f\"  ‚ùå None of the target columns found in {building_type.lower()} data\")\n",
    "        return\n",
    "    \n",
    "    print(f\"  ‚úì Analyzing columns: {available_columns}\")\n",
    "    \n",
    "    # Create subplots for distributions\n",
    "    n_cols = len(available_columns)\n",
    "    if n_cols == 0:\n",
    "        return\n",
    "    \n",
    "    # Calculate subplot layout\n",
    "    n_rows = (n_cols + 2) // 3  # Max 3 columns per row\n",
    "    n_subplot_cols = min(3, n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_subplot_cols, figsize=(5*n_subplot_cols, 4*n_rows))\n",
    "    \n",
    "    # Handle single subplot case\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes if n_cols > 1 else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Analyze each column\n",
    "    for i, column in enumerate(available_columns):\n",
    "        ax = axes[i]\n",
    "        data = buildings_gdf[column].dropna()\n",
    "        \n",
    "        print(f\"\\nüìà {column.upper()} ANALYSIS:\")\n",
    "        print(f\"  - Total valid values: {len(data):,}\")\n",
    "        print(f\"  - Missing values: {buildings_gdf[column].isna().sum():,}\")\n",
    "        \n",
    "        if len(data) == 0:\n",
    "            ax.text(0.5, 0.5, f'No data for\\n{column}', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{column}')\n",
    "            continue\n",
    "        \n",
    "        # Handle different data types\n",
    "        if column == 'floor_area':\n",
    "            # Bin floor areas for better visualization\n",
    "            bins = [0, 50, 100, 200, 500, 1000, 2000, 5000, float('inf')]\n",
    "            labels = ['<50', '50-100', '100-200', '200-500', '500-1k', '1k-2k', '2k-5k', '>5k']\n",
    "            \n",
    "            try:\n",
    "                binned_data = pd.cut(data, bins=bins, labels=labels, include_lowest=True)\n",
    "                value_counts = binned_data.value_counts().sort_index()\n",
    "                \n",
    "                print(f\"  - Mean area: {data.mean():.1f} m¬≤\")\n",
    "                print(f\"  - Median area: {data.median():.1f} m¬≤\")\n",
    "                print(f\"  - Area range: {data.min():.1f} - {data.max():.1f} m¬≤\")\n",
    "                \n",
    "                # Distribution\n",
    "                print(f\"  - Area distribution:\")\n",
    "                for area_range, count in value_counts.items():\n",
    "                    pct = (count / len(data)) * 100\n",
    "                    print(f\"    * {area_range} m¬≤: {count:,} ({pct:.1f}%)\")\n",
    "                \n",
    "                value_counts.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "                ax.set_title(f'{column} Distribution (m¬≤)')\n",
    "                ax.set_xlabel('Area Range (m¬≤)')\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Error binning floor area: {e}\")\n",
    "                data.hist(bins=20, ax=ax, alpha=0.7, edgecolor='black')\n",
    "                ax.set_title(f'{column} Distribution')\n",
    "        \n",
    "        elif data.dtype in ['object', 'string']:\n",
    "            # Categorical data\n",
    "            value_counts = data.value_counts().head(10)  # Top 10 categories\n",
    "            \n",
    "            print(f\"  - Unique values: {data.nunique()}\")\n",
    "            print(f\"  - Distribution:\")\n",
    "            for value, count in value_counts.items():\n",
    "                pct = (count / len(data)) * 100\n",
    "                print(f\"    * {value}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            value_counts.plot(kind='bar', ax=ax, color='lightgreen', edgecolor='black')\n",
    "            ax.set_title(f'{column} Distribution')\n",
    "            ax.set_xlabel(column)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        else:\n",
    "            # Numeric data\n",
    "            try:\n",
    "                print(f\"  - Mean: {data.mean():.2f}\")\n",
    "                print(f\"  - Median: {data.median():.2f}\")\n",
    "                print(f\"  - Range: {data.min()} - {data.max()}\")\n",
    "                \n",
    "                # Show value counts if reasonable number of unique values\n",
    "                if data.nunique() <= 20:\n",
    "                    value_counts = data.value_counts().sort_index()\n",
    "                    print(f\"  - Distribution:\")\n",
    "                    for value, count in value_counts.items():\n",
    "                        pct = (count / len(data)) * 100\n",
    "                        print(f\"    * {value}: {count:,} ({pct:.1f}%)\")\n",
    "                    \n",
    "                    value_counts.plot(kind='bar', ax=ax, color='orange', edgecolor='black')\n",
    "                    ax.set_title(f'{column} Distribution')\n",
    "                    ax.set_xlabel(column)\n",
    "                else:\n",
    "                    # Too many unique values, use histogram\n",
    "                    data.hist(bins=20, ax=ax, alpha=0.7, edgecolor='black')\n",
    "                    ax.set_title(f'{column} Distribution')\n",
    "                    ax.set_xlabel(column)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Error analyzing numeric data: {e}\")\n",
    "                data.hist(bins=20, ax=ax, alpha=0.7, edgecolor='black')\n",
    "                ax.set_title(f'{column} Distribution')\n",
    "        \n",
    "        ax.set_ylabel('Count')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(len(available_columns), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_final_building_classifications(region_path: Path) -> None:\n",
    "    \"\"\"Main function to analyze final building classification outputs.\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è FINAL BUILDING CLASSIFICATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load building shapefiles\n",
    "    building_data = load_building_shapefiles(region_path)\n",
    "    \n",
    "    if not building_data:\n",
    "        print(\"No building shapefiles found for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Define columns to analyze for each building type\n",
    "    residential_columns = [\n",
    "        'floor_area', 'building_u', 'free_walls', \n",
    "        'building_t', 'occupants', 'floors', 'constructi'\n",
    "    ]\n",
    "    \n",
    "    non_residential_columns = [\n",
    "        'floor_area', 'building_use', 'free_walls'\n",
    "    ]\n",
    "    \n",
    "    # Analyze residential buildings\n",
    "    if 'residential' in building_data:\n",
    "        analyze_building_distributions(\n",
    "            building_data['residential'], \n",
    "            \"Residential\", \n",
    "            residential_columns\n",
    "        )\n",
    "    \n",
    "    # Analyze non-residential buildings\n",
    "    if 'non_residential' in building_data:\n",
    "        analyze_building_distributions(\n",
    "            building_data['non_residential'], \n",
    "            \"Non-Residential\", \n",
    "            non_residential_columns\n",
    "        )\n",
    "    \n",
    "    # Summary comparison\n",
    "    if 'residential' in building_data and 'non_residential' in building_data:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"BUILDING CLASSIFICATION SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        res_count = len(building_data['residential'])\n",
    "        non_res_count = len(building_data['non_residential'])\n",
    "        total_count = res_count + non_res_count\n",
    "        \n",
    "        print(f\"\\nüìä OVERALL STATISTICS:\")\n",
    "        print(f\"  - Total buildings classified: {total_count:,}\")\n",
    "        print(f\"  - Residential buildings: {res_count:,} ({(res_count/total_count)*100:.1f}%)\")\n",
    "        print(f\"  - Non-residential buildings: {non_res_count:,} ({(non_res_count/total_count)*100:.1f}%)\")\n",
    "        \n",
    "        # Create summary comparison chart\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        \n",
    "        categories = ['Residential', 'Non-Residential']\n",
    "        counts = [res_count, non_res_count]\n",
    "        colors = ['lightblue', 'lightcoral']\n",
    "        \n",
    "        bars = ax.bar(categories, counts, color=colors, edgecolor='black')\n",
    "        ax.set_title('Building Classification Summary')\n",
    "        ax.set_ylabel('Number of Buildings')\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + total_count*0.01,\n",
    "                   f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Execute the analysis\n",
    "analyze_final_building_classifications(REGION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Road Network Analysis TODO\n",
    "\n",
    "This section evaluates the road network generation from Step 5, including:\n",
    "- Network topology and connectivity\n",
    "- Routable network validation\n",
    "- Spatial coverage and completeness\n",
    "\n",
    "**Key Outputs to Validate:**\n",
    "- Generated road network file (GPKG/GeoJSON)\n",
    "- Network connectivity and routing capability\n",
    "- Integration with regional boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
